# -*- coding: utf-8 -*-
"""SMS Spam Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/143hkKjHKV_GbProrJuiwXfKEQwZi7Tyd

importing the Dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

"""Importing the data into pandas dataframe"""

sms = pd.read_csv('spam.csv', encoding='latin-1')
sms.head()

"""checking null values"""

sms.isnull().sum()

sms.dropna(how="any", inplace=True, axis=1)
sms.columns = ['label', 'message']
sms.head(10)

""" Exploratory Data Analysis (EDA)

"""

sms.describe()

sms.groupby('label').describe()

"""We have 4825 ham message and 747 spam message"""

# convert label to a numerical variable
sms['label_num'] = sms.label.map({'ham':0, 'spam':1})
sms.head()

# adding a message length feature
sms['message_len'] = sms.message.apply(len)
sms.head()

plt.figure(figsize=(10, 6))
sms[sms.label == 'ham'].message_len.plot(bins=35, kind='hist', color='blue', label='Ham messages', alpha=0.6)
sms[sms.label == 'spam'].message_len.plot(kind='hist', color='red', label='Spam messages', alpha=0.6)

plt.legend()
plt.xlabel('Message Length')

"""Spam messages tend to have more characters and Ham messages are more than Spam ones"""

sms[sms.label=='ham'].describe()

sms[sms.label=='spam'].describe()

sms[sms.message_len == 910].message.iloc[0]

"""##Text Pre-processing

"""

#removing punctuations and stopwords

import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

def text_process(text):

  text = text.lower()
  non_punc = [char for char in text if char not in string.punctuation]
  non_punc = ''.join(non_punc)

  STOPWORDS = stopwords.words('english') + ['u', 'Ã¼', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']
  lemmatizer = WordNetLemmatizer()

  cleaned_words = [lemmatizer.lemmatize(word) for word in non_punc.split() if word not in STOPWORDS]
  return ' '.join(cleaned_words)

sms.head()

sms['clean_msg'] = sms.message.apply(text_process)

sms.head()

from collections import Counter

ham_messages = sms[sms.label == 'ham'].clean_msg.apply(lambda x: [word for word in x.split()])
c = Counter()

for msg in ham_messages:
  c.update(msg)

print(c.most_common(20))

spam_messages = sms[sms.label == 'spam'].clean_msg.apply(lambda x: [word for word in x.split()])
c_ = Counter()

for msg in spam_messages:
  c_.update(msg)

print(c_.most_common(20))

"""splitting the features and Labels and Vectorizing the cleaned texts"""

from sklearn.model_selection import train_test_split

X = sms.clean_msg
y = sms.label_num

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

from sklearn.feature_extraction.text import TfidfVectorizer
vect = TfidfVectorizer()

X_train_dtm = vect.fit_transform(X_train)
X_test_dtm = vect.transform(X_test)

"""##Building and evaluating a model"""

# Commented out IPython magic to ensure Python compatibility.
# Multinomial Naive Base
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

# %time nb.fit(X_train_dtm, y_train)

# Evaluating the model

from sklearn import metrics
y_pred = nb.predict(X_test_dtm)

print("=======Accuracy Score===========")
print(metrics.accuracy_score(y_pred, y_test))
print("=======Confision Matrix===========")
print(metrics.confusion_matrix(y_test, y_pred))

# printing the false negative
X_test[y_pred < y_test]

# Commented out IPython magic to ensure Python compatibility.
# logistic regression modeling
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()

# %time logreg.fit(X_train_dtm, y_train)

# Evaluating the model
y_pred = logreg.predict(X_test_dtm)
print("=======Accuracy Score===========")
print(metrics.accuracy_score(y_test, y_pred))

# print the confusion matrix
print("=======Confision Matrix===========")
print(metrics.confusion_matrix(y_test, y_pred))

# Commented out IPython magic to ensure Python compatibility.
# Xgboost
from xgboost import XGBClassifier
xgb = XGBClassifier()

# %time xgb.fit(X_train_dtm, y_train)

# Evaluating the model
y_pred = xgb.predict(X_test_dtm)

print("=======Accuracy Score===========")
print(metrics.accuracy_score(y_test, y_pred))

# print the confusion matrix
print("=======Confision Matrix===========")
print(metrics.confusion_matrix(y_test, y_pred))

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'solver': ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga'],
    'max_iter': [50, 100, 200, 300, 500, 1000],
    'class_weight': [None, 'balanced']
}

grid_search = GridSearchCV(logreg, param_grid, cv=5, n_jobs=-1)

# %time grid_search.fit(X_train_dtm, y_train)

print("Best hyperparameters:", grid_search.best_params_)

y_pred = grid_search.predict(X_test_dtm)
print("=======Accuracy Score===========")
print(metrics.accuracy_score(y_test, y_pred))

# print the confusion matrix
print("=======Confision Matrix===========")
print(metrics.confusion_matrix(y_test, y_pred))

"""Using grid search we achieved a 99% accuracy which is a very solid result"""

