# -*- coding: utf-8 -*-
"""Movie_Genre_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M2YtYKyONEScQ34edpwgDEfALWcXwrbr

# Text Classification of Movie Plots to Predict Movie Genre

Importing The Dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib  inline

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import re
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
import string
import spacy
import en_core_web_lg
nlp = en_core_web_lg.load()

"""Read data into pandas"""

#download datasets from https://www.kaggle.com/datasets/hijest/genre-classification-dataset-imdb
df = pd.read_csv('/content/wiki_movie_plots_deduped.csv')
df.head(10)

"""Descriptive Statistics"""

df.describe()

"""There are 34886 rows and 8 columns"""

df.shape

df.info()

"""Exploratory Data Analysis

Movie Distributions by Release Year
"""

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Release Year')
plt.title('Frequency of Release Years')
plt.xlabel('Realease Year')
plt.ylabel('Frequency of Occurence')

"""Movie distribution by Movie Origin"""

plt.figure(figsize=(10,6))
sns.countplot(df, x='Origin/Ethnicity')
plt.xticks(rotation = 90)
plt.title('Distribution of Movie Origin')
plt.xlabel('Movie Origin')
plt.ylabel('Frequency Distribution')

"""Removing columns with unknown Genres and reseting index




"""

df = df[df['Genre'] != 'unknown']
df.reset_index(drop=True, inplace=True)
df.head()

"""Checking the number of unique values we have in the genres column


"""

genres=pd.value_counts(df.Genre)

print('There are ',len(genres), 'different Genres in the dataset:')
print('-'*50)
print(genres)

"""Getting the top 6 most commonly occuring genres in the dataset"""

top_genres = pd.DataFrame(genres[:6]).reset_index()
top_genres.columns = ['genres', 'number_of_movies']
top_genres

"""Movie distribution of the top six genres in the dataset"""

sns.barplot(data=top_genres, x='genres', y='number_of_movies')
plt.title('Top 6 genres and their frequency')
plt.xlabel('genres')
plt.ylabel('frequency')

"""Creating separate labels column for the top 6 genres only"""

conditions = [df['Genre']=='drama', df['Genre']=='comedy', df['Genre']=='horror', df['Genre']=='action', df['Genre']=='thriller',
             df['Genre']=='romance']
choices = [1,2,3,4,5,6]
df['labels'] = np.select(conditions, choices, 0)

df.sample(3)

df['labels'].value_counts()

df_to_use = (df.loc[df['labels']!=0]).reset_index(drop=True)
 df_to_use

plt.figure(figsize=(10, 6))
sns.histplot(x = 'Release Year', hue = 'Genre', data = df_to_use, multiple = 'stack')
plt.title('Frequency of Release Years Showing Genres')
plt.xlabel('Release Year')
plt.ylabel('Frequency of Occurence')

"""Dropping unnecessary columns"""

df_to_use.drop(columns = ['Release Year', 'Origin/Ethnicity', 'Director', 'Cast', 'Wiki Page'], axis = 1, inplace = True)
df_to_use.head()

"""## Cleaning up the 'Plot' column for Analysis"""

df_to_use['Plot'][2]

"""## Total number of words in the Plot summaries"""

def word_length(text):
    num_of_words = 0
    for row in text:
        words = [word for word in row.split(' ')]
        num_of_words += len(words)
    return num_of_words

raw_length = word_length(df_to_use['Plot'])
print(raw_length)

"""## Total number of UNIQUE words in the plot summaries"""

def unique_word_length(text):
    unique_words = set()
    for row in text:
        words = [word for word in row.split(' ')]
        unique_words.update(words)
    return len(unique_words)

raw_unique_length = unique_word_length(df_to_use['Plot'])
print(raw_unique_length)

"""## Creating our stopwords list and adding some more words that are very common in the summaries. Changing the text to lower case, stopwords removal, lemmatizing."""

stopwords_list = stopwords.words('english')
stopwords_list += list(string.punctuation)
stopwords_list += ['one', 'two', 'go','goes', 'get', 'also', 'however', 'tells']
stopwords_list += [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

def clean_text(text):
    text = text.lower()
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "can not ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r"\'scuse", " excuse ", text)
    text = re.sub('\W', ' ', text)
    text = re.sub('\s+', ' ', text)
    text = re.sub('\d+', ' ', text)
    text = text.strip(' ')
    text = word_tokenize(text)
    text = ' '.join(text)
    text = nlp(text)
    text = [w.lemma_ for w in text]
    text = [w for w in text if w not in stopwords_list]
    text = ' '.join(text)
    return text

new_df = df_to_use.copy(deep = True)

new_df['Plot'] = new_df['Plot'].map(lambda x : clean_text(x))
new_df['Plot'][5]

"""## Clean word length vs without cleaning word length Comparison"""

clean_length = word_length(new_df['Plot'])
clean_unique_length = unique_word_length(new_df['Plot'])

print(f'clean length is {clean_length} vs Raw lenght is {raw_length}')
print(f'clean unique length is {clean_unique_length} vs Raw unique lenght is {raw_unique_length}')

"""# Modeling

Assigning Independent and Target Variables and performing train test split into training and testing set
"""

X = new_df['Plot']
y = new_df['labels']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state = 42, stratify = y)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

"""Using TfidfVectorizer on the independent variable and testing out the Multinomial Naive Bayes Model, Logistic Regression Model and Decision Tree

"""

vectorizer = TfidfVectorizer(ngram_range = (1,1), max_df=.85, min_df=15, lowercase=False)
tfidf_Xtr = vectorizer.fit_transform(X_train)

"""Multinomial Naive Bayes gave accuracy of ~56%"""

mnb = MultinomialNB()
mnb.fit(tfidf_Xtr, y_train)
accuracy_score(mnb.predict(vectorizer.transform(X_test)), y_test)

"""Logistic Regression gave accuracy of ~64%"""

lr = LogisticRegression(max_iter=1000)
lr.fit(tfidf_Xtr, y_train)
accuracy_score(lr.predict(vectorizer.transform(X_test)), y_test)

"""Desicion Tree gave an accuracy of 43%"""

dtc = DecisionTreeClassifier()
dtc.fit(tfidf_Xtr, y_train)
accuracy_score(dtc.predict(vectorizer.transform(X_test)), y_test)

"""Random forest gave an accuracy of 55%"""

rfc = RandomForestClassifier()
rfc.fit(tfidf_Xtr, y_train)
accuracy_score(rfc.predict(vectorizer.transform(X_test)), y_test)

"""## XGBOOST classifier gace an accuracy of 61%"""

from xgboost import XGBClassifier
xgb = XGBClassifier()
xgb.fit(tfidf_Xtr, y_train-1)
accuracy_score(xgb.predict(vectorizer.transform(X_test)), y_test-1)