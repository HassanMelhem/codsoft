# -*- coding: utf-8 -*-
"""Credit_Card_Fraud_Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16amYpOe1HNlkMrkH0TuxTeCvCMHYZ_a2

# Credit Card Fraud Detection

Imoporting the Dependacies
"""

import numpy as np
import pandas as pd
import seaborn as sns
import plotly.express as px
from matplotlib import pyplot as plt

from sklearn import metrics
pd.set_option('display.max_columns', None)
sns.set(rc = {'figure.figsize':(15,8)})

"""Import Data"""

# download the dataset from https://www.kaggle.com/datasets/kartik2112/fraud-detection
train_df = pd.read_csv('fraudTrain.csv', index_col=0)
train_df.head()

test_df = pd.read_csv('fraudTest.csv', index_col=0)
test_df.head()

"""### Data imbalance check"""

labels=["Genuine","Fraud"]
train_df['is_fraud'].value_counts()

fig = px.pie(values=train_df['is_fraud'].value_counts(), names=labels , color_discrete_sequence=["skyblue","black"]
             ,title="Fraud vs Genuine transactions")
fig.show()

plt.figure(figsize=(3,4))
sns.countplot(train_df, x='is_fraud')

print('Genuine:', round(train_df['is_fraud'].value_counts()[0]/len(train_df) * 100,2), '% of the dataset')
print('Frauds:', round(train_df['is_fraud'].value_counts()[1]/len(train_df) * 100,2), '% of the dataset')

"""Dropping duplicated values"""

train_df.drop_duplicates(inplace=True)

"""Checking null values"""

train_df.isnull().sum()

"""##Exploratory Data Analysis"""

train_df.amt.describe()

"""Transaction amount vs Fraud"""

ax=sns.histplot(x='amt',data=train_df[train_df.amt<1000],hue='is_fraud',stat='percent',multiple='dodge', common_norm=False, bins=30)
ax.set_ylabel('Percentage in Each Type')
ax.set_xlabel('Transaction Amount in USD')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

"""fraudulent transactions peak around \$300 and then at the $800-\$1000 range"""

ax=sns.histplot(x='gender',data=train_df, hue='is_fraud', multiple='dodge', stat='percent',common_norm=False)
ax.set_ylabel('Percentage')
ax.set_xlabel('Credit Card Holder Gender')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

"""we do not see a clear difference between both genders.

Spending Category vs Fraud
"""

not_fraud_category = train_df[train_df['is_fraud']==0]['category'].value_counts(normalize=True).to_frame().reset_index()
not_fraud_category.columns = ['category','not fraud percentage']\

fraud_category = train_df[train_df['is_fraud']==1]['category'].value_counts(normalize=True).to_frame().reset_index()
fraud_category.columns = ['category', 'fraud percentage']

merged_df = not_fraud_category.merge(fraud_category, on='category')
merged_df['diff'] = merged_df['fraud percentage']-merged_df['not fraud percentage']

ax=sns.barplot(y='category',x='diff',data=merged_df.sort_values('diff',ascending=False))
ax.set_xlabel('Percentage Difference')
ax.set_ylabel('Transaction Category')
plt.title('The Percentage Difference of Fraudulent over Non-Fraudulent Transations in Each Spending Category ')

"""Fraud tends to happen more often in 'Shopping_net', 'Grocery_pos', and 'misc_net' while 'home' and 'kids_pets' among others tend to see more normal transactions than fraudulent ones.

Hourly Trend
"""

train_df['hour']=pd.to_datetime(train_df['trans_date_trans_time']).dt.hour
ax=sns.histplot(data=train_df, x="hour", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Time (Hour) in a Day')
plt.xticks(np.arange(0,24,1))
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

"""fraudulent payments happen disproportionately around midnight when most people are asleep!

Day of the week vs fraud
"""

train_df['day'] = pd.to_datetime(train_df['trans_date_trans_time']).dt.dayofweek
ax = sns.histplot(data=train_df, x="day", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_xticklabels(['', "Mon","Tue","Wed","Thu","Fri","Sat","Sun"])
ax.set_ylabel('Percentage')
ax.set_xlabel('Day of Week')
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

"""Normal transactions tend to happen more often on Monday and Sunday while fraudulent ones tend to spread out more evenly throughout the week.

month vs fraud
"""

train_df['month']=pd.to_datetime(train_df['trans_date_trans_time']).dt.month
ax=sns.histplot(data=train_df, x="month", hue="is_fraud", common_norm=False,stat='percent',multiple='dodge')
ax.set_ylabel('Percentage')
ax.set_xlabel('Month')
plt.xticks(np.arange(1,13,1))
ax.set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun","Jul",'Aug','Sep','Oct','Nov','Dec'])
plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])

"""fraudulent transactions are more concentrated in Jan-May

State vs Fraud
"""

a=train_df['state'][train_df.is_fraud==0].value_counts(normalize=True)
a=a.to_frame()
a=a.reset_index()
a.columns = ['State', 'Per']

b=train_df['state'][train_df.is_fraud==1].value_counts(normalize=True)
b=b.to_frame()
b=b.reset_index()
b.columns = ['State', 'Per']
merged=a.merge(b,on='State')
merged['diff']=merged['Per_y']-merged['Per_x']
merged['diff']=merged['diff']*100
merged=merged.sort_values('diff',ascending=False)

ax1=sns.barplot(data=merged, x='diff',y='State')
ax1.set_xlabel('Percentage Difference')
ax1.set_ylabel('State')
plt.title('The Percentage of Fraudulent over Non-Fraudulent Transcations in Each State')

"""As can be seen, NY and OH among others have a higher percentage of fraudulent transactions

##Data Modeling and Prediction

feature chosen based on EDA
"""

import datetime as dt

train_df['age'] = dt.date.today().year - pd.to_datetime(train_df['trans_date_trans_time']).dt.year

train=train_df[['category','amt','zip','lat','long','city_pop','merch_lat','merch_long','age','hour','day','month','is_fraud']]
train.head()

train=pd.get_dummies(train, drop_first=True)
train.head()

y_train=train['is_fraud'].values
X_train=train.drop("is_fraud", axis=1).values

"""apply same on test data"""

test_df['age']=dt.date.today().year-pd.to_datetime(test_df['dob']).dt.year
test_df['hour']=pd.to_datetime(test_df['trans_date_trans_time']).dt.hour
test_df['day']=pd.to_datetime(test_df['trans_date_trans_time']).dt.dayofweek
test_df['month']=pd.to_datetime(test_df['trans_date_trans_time']).dt.month

test_df=test_df[['category','amt','zip','lat','long','city_pop','merch_lat','merch_long','age','hour','day','month','is_fraud']]
test_df=pd.get_dummies(test_df, drop_first=True)
y_test=test_df['is_fraud'].values
X_test=test_df.drop("is_fraud", axis='columns').values

from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

"""###logistic regression modeling and metrics"""

method= SMOTE()
X_resampled, y_resampled = method.fit_resample(X_train, y_train)
model=LogisticRegression()
model.fit(X_resampled,y_resampled)
predicted=model.predict(X_test)
print('Classification report:\n', classification_report(y_test, predicted))
conf_mat = confusion_matrix(y_test, predicted)
print('Confusion matrix:\n', conf_mat)

"""###Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
model2 = RandomForestClassifier(random_state=5)
model2.fit(X_resampled,y_resampled)
predicted=model2.predict(X_test)
print('Classification report:\n', classification_report(y_test, predicted))
conf_mat = confusion_matrix(y_true=y_test, y_pred=predicted)
print('Confusion matrix:\n', conf_mat)

"""###XGBOOST classifier"""

from xgboost import XGBClassifier
model3 = XGBClassifier()
model3.fit(X_resampled,y_resampled)
predicted=model3.predict(X_test)
print('Classification report:\n', classification_report(y_test, predicted))
conf_mat = confusion_matrix(y_true=y_test, y_pred=predicted)
print('Confusion matrix:\n', conf_mat)

